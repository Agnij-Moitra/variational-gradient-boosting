{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "# from VGBoost import VGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import DataFrame, concat\n",
    "from numba import prange\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "# Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, f1_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, BaggingRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import NuSVR, SVC\n",
    "from sklearn.base import BaseEstimator\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge, ElasticNet, SGDRegressor, LassoLars, Lasso, Ridge, ARDRegression, RANSACRegressor, HuberRegressor, TheilSenRegressor, LassoLarsIC\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.impute import KNNImputer\n",
    "from time import perf_counter\n",
    "import collections.abc\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class VGBRegressor(BaseEstimator):\n",
    "    \"\"\"_summary_\n",
    "    Args:\n",
    "        object (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Initialize VGBRegressor Object\n",
    "        \"\"\"\n",
    "        self._ensemble = []\n",
    "\n",
    "    def _metrics(self, vt, vp, model, time=None):\n",
    "        \"\"\"_summary_\n",
    "        Args:\n",
    "            vt (_type_): _description_\n",
    "            vp (_type_): _description_\n",
    "            model (_type_): _description_\n",
    "            time (_type_, optional): _description_. Defaults to None.\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        if self.custom_loss_metrics:\n",
    "            return {'model': model, 'time': time, 'loss': self.custom_loss_metrics(vt, vp)}\n",
    "        return {\"model\": model, \"time\": time, \"loss\": mean_squared_error(vt, vp)}\n",
    "\n",
    "    def _create_model(self, X, y, model_name, time_it: bool = False):\n",
    "        \"\"\"_summary_\n",
    "        Args:\n",
    "            X (_type_): _description_\n",
    "            y (_type_): _description_\n",
    "            model_name (_type_): _description_\n",
    "            time_it (bool, optional): _description_. Defaults to False.\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        model = model_name()\n",
    "        if time_it:\n",
    "            begin = perf_counter()\n",
    "            model.fit(X, y)\n",
    "            end = perf_counter()\n",
    "            return (model, end - begin)\n",
    "        return (model.fit(X, y), None)\n",
    "\n",
    "    def _get_metrics(self, model_name):\n",
    "        \"\"\"_summary_\n",
    "        Args:\n",
    "            model_name (_type_): _description_\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        try:\n",
    "            Xt, Xv, yt, yv = train_test_split(self._X, self._y)\n",
    "            results = self._create_model(Xt, yt, model_name, time_it=False)\n",
    "            model, time = results[0], results[1]\n",
    "            return self._metrics(yv,\n",
    "                                 model.predict(Xv), model, time)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _get_results(self, X, y) -> list:\n",
    "        \"\"\"_summary_\n",
    "        Args:\n",
    "            X (_type_): _description_\n",
    "            y (_type_): _description_\n",
    "        Returns:\n",
    "            list: _description_\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        # self._X = self._minimax.fit_transform(self._robust.fit_transform(\n",
    "        #         KNNImputer(weights='distance').fit_transform(X)))\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        with ThreadPoolExecutor(max_workers=len(self._models)) as executor:\n",
    "            res = executor.map(self._get_metrics, self._models)\n",
    "            results = [i for i in res if i]\n",
    "        return results\n",
    "\n",
    "    def fit(\n",
    "        self, X_train, y_train,\n",
    "        early_stopping: bool = False,\n",
    "        early_stopping_min_delta: float = 0.001,\n",
    "        early_stopping_patience: int = 10,\n",
    "        custom_models: list = None,\n",
    "        learning_rate: float = 0.05,\n",
    "        n_estimators: int = 100,\n",
    "        warm_start: bool = False,\n",
    "        complexity: bool = False,\n",
    "        light: bool = True,\n",
    "        custom_loss_metrics: object = False,\n",
    "    ):\n",
    "        \"\"\"_summary_\n",
    "        Args:\n",
    "            X_train (_type_): _description_\n",
    "            y_train (_type_): _description_\n",
    "            early_stopping (bool, optional): _description_. Defaults to False.\n",
    "            early_stopping_min_delta (float, optional): _description_. Defaults to 0.001.\n",
    "            early_stopping_patience (int, optional): _description_. Defaults to 10.\n",
    "            custom_models (list, optional): _description_. Defaults to None.\n",
    "            learning_rate (float, optional): _description_. Defaults to 0.05.\n",
    "            n_estimators (int, optional): _description_. Defaults to 100.\n",
    "            warm_start (bool, optional): _description_. Defaults to False.\n",
    "            complexity (bool, optional): _description_. Defaults to False.\n",
    "            custom_loss_metrics (object, optional): _description_. Defaults to False.\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        if custom_models:\n",
    "            self._models = custom_models\n",
    "        self.custom_loss_metrics = custom_loss_metrics\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.early_stopping = early_stopping\n",
    "        self.early_stopping_min_delta = early_stopping_min_delta\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        if custom_models:\n",
    "            \n",
    "            self._models = custom_models\n",
    "        else:\n",
    "            if complexity:\n",
    "                self._models = (DecisionTreeRegressor, LinearRegression, BayesianRidge, KNeighborsRegressor, HistGradientBoostingRegressor,\n",
    "                                ElasticNet, LassoLars, Lasso, GradientBoostingRegressor, ExtraTreesRegressor,\n",
    "                                BaggingRegressor, NuSVR, XGBRegressor, SGDRegressor, KernelRidge, MLPRegressor, LGBMRegressor,\n",
    "                                Ridge, ARDRegression, RANSACRegressor, HuberRegressor, TheilSenRegressor, LassoLarsIC)\n",
    "            elif light:\n",
    "                self._models = (LGBMRegressor, ExtraTreesRegressor,\n",
    "                                BaggingRegressor, RANSACRegressor, LassoLarsIC, BayesianRidge)\n",
    "            else:\n",
    "                self._models = (DecisionTreeRegressor, LinearRegression, BayesianRidge, KNeighborsRegressor, LGBMRegressor,\n",
    "                                ElasticNet, LassoLars, Lasso, SGDRegressor, BaggingRegressor, ExtraTreesRegressor,\n",
    "                                Ridge, ARDRegression, RANSACRegressor, LassoLarsIC)\n",
    "        X_train = KNNImputer(weights='distance',\n",
    "                             n_neighbors=10).fit_transform(deepcopy(X_train))\n",
    "        self._y_mean = y_train.mean()\n",
    "        # base model: mean\n",
    "        # computer residuals: y - y hat\n",
    "        # for n_estimators: a) y = prev residuals && residuals * learning rate\n",
    "        # add early stopping\n",
    "        # restore best weights\n",
    "        # ada boost and adaptive scaling for learning rates\n",
    "\n",
    "        preds = DataFrame(\n",
    "            data={'yt': y_train, 'p0': np.full((len(y_train)), y_train - self._y_mean)})\n",
    "        residuals = DataFrame(\n",
    "            data={'r0': y_train - self._y_mean})\n",
    "        errors = []\n",
    "        if not early_stopping:\n",
    "            if warm_start:\n",
    "                for i in prange(1, self.n_estimators + 1):\n",
    "                    y = residuals[f'r{i - 1}']\n",
    "                    results = self._get_results(X_train, y)\n",
    "                    min_loss = min(results, key=lambda x: x.get(\n",
    "                        \"loss\", float('inf')))[\"loss\"]  # https://stackoverflow.com/a/19619294\n",
    "                    min_model = [i['model']\n",
    "                                 for i in results if min_loss >= i['loss']][0]\n",
    "                    preds[f'p{i}'] = residuals.sum(axis=1) + min_model.predict(\n",
    "                        X_train) * self.learning_rate\n",
    "                    residuals[f'r{i}'] = preds['yt'] - preds[f'p{i}']\n",
    "                    if i % 3 == 0:\n",
    "                        X_train[f\"r{i}\"] = residuals[f'r{i}'].copy()\n",
    "                    try:\n",
    "                        errors.append(mean_squared_error(\n",
    "                            preds['yt'], preds[f'p{i}']))\n",
    "                    except Exception:\n",
    "                        df = concat(\n",
    "                            [preds['yt'], preds[f'p{i - 1}']], axis=1).dropna()\n",
    "                        errors.append(mean_squared_error(\n",
    "                            df['yt'], df[f\"p{i - 1}\"]))\n",
    "                    self._ensemble.append(min_model)\n",
    "            else:\n",
    "                for i in prange(1, self.n_estimators + 1):\n",
    "                    y = residuals[f'r{i - 1}']\n",
    "                    results = self._get_results(X_train, y)\n",
    "                    min_loss = min(results, key=lambda x: x.get(\n",
    "                        \"loss\", float('inf')))[\"loss\"]  # https://stackoverflow.com/a/19619294\n",
    "                    min_model = [i['model']\n",
    "                                 for i in results if min_loss >= i['loss']][0]\n",
    "                    preds[f'p{i}'] = residuals.sum(axis=1) + min_model.predict(\n",
    "                        X_train) * self.learning_rate\n",
    "                    residuals[f'r{i}'] = preds['yt'] - preds[f'p{i}']\n",
    "                    errors.append(mean_squared_error(\n",
    "                        preds['yt'], preds[f'p{i}']))\n",
    "                    self._ensemble.append(min_model)\n",
    "                    if errors[i - 1] == 0:\n",
    "                        break\n",
    "        else:\n",
    "            return \"TODO\"\n",
    "        min_error = min(errors)\n",
    "        min_error_i = [i for i in prange(\n",
    "            len(errors)) if errors[i] == min_error][0]\n",
    "        self._ensemble, errors = self._ensemble[:\n",
    "                                                min_error_i], errors[:min_error_i]\n",
    "        residuals = residuals[:len(errors)]\n",
    "        return self._ensemble, (residuals, errors)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"_summary_\n",
    "        Args:\n",
    "            X_test (_type_): _description_\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        try:\n",
    "            val = self._ensemble[0]\n",
    "        except Exception:\n",
    "            return \"Please train the model first\"\n",
    "        # X_test = self._robust.transform(self._minimax.transform(deepcopy(X_test)))\n",
    "        preds = DataFrame(\n",
    "            data={'p0': np.full((len(X_test)), self._y_mean)})\n",
    "        for i in prange(len(self._ensemble)):\n",
    "            preds[f\"p{i}\"] = self._ensemble[i].predict(X_test)\n",
    "        preds_ = preds.sum(axis=1)\n",
    "        return preds_\n",
    "\n",
    "    def score(self, X_test, y_true):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_test (Iterable)\n",
    "            y_true (Iterable)\n",
    "        Returns:\n",
    "            float: R2 Score for y_true and y_predicted\n",
    "        \"\"\"\n",
    "        return r2_score(y_true, self.predict(X_test))\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.__dict__\n",
    "\n",
    "\n",
    "class VGBClassifier(BaseEstimator):\n",
    "    \"\"\"_summary_\n",
    "    Args:\n",
    "        object (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Initialize VGBRegressor Object\n",
    "        \"\"\"\n",
    "        self._ensemble = []\n",
    "\n",
    "    def _metrics(self, vt, vp, model, time=None):\n",
    "        \"\"\"_summary_\n",
    "        Args:\n",
    "            vt (_type_): _description_\n",
    "            vp (_type_): _description_\n",
    "            model (_type_): _description_\n",
    "            time (_type_, optional): _description_. Defaults to None.\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        if self.custom_loss_metrics:\n",
    "            return {'model': model, 'time': time, 'loss': self.custom_loss_metrics(vt, vp)}\n",
    "        return {\"model\": model, \"time\": time, \"loss\": mean_squared_error(vt, vp)}\n",
    "\n",
    "    def _create_model(self, X, y, model_name, time_it: bool = False):\n",
    "        \"\"\"_summary_\n",
    "        Args:\n",
    "            X (_type_): _description_\n",
    "            y (_type_): _description_\n",
    "            model_name (_type_): _description_\n",
    "            time_it (bool, optional): _description_. Defaults to False.\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        model = model_name()\n",
    "        if time_it:\n",
    "            begin = perf_counter()\n",
    "            model.fit(X, y)\n",
    "            end = perf_counter()\n",
    "            return (model, end - begin)\n",
    "        return (model.fit(X, y), None)\n",
    "\n",
    "    def _get_metrics(self, model_name):\n",
    "        \"\"\"_summary_\n",
    "        Args:\n",
    "            model_name (_type_): _description_\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        try:\n",
    "            Xt, Xv, yt, yv = train_test_split(self._X, self._y)\n",
    "            results = self._create_model(Xt, yt, model_name, time_it=False)\n",
    "            model, time = results[0], results[1]\n",
    "            return self._metrics(yv,\n",
    "                                 model.predict(Xv), model, time)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _get_results(self, X, y) -> list:\n",
    "        \"\"\"_summary_\n",
    "        Args:\n",
    "            X (_type_): _description_\n",
    "            y (_type_): _description_\n",
    "        Returns:\n",
    "            list: _description_\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        # self._X = self._minimax.fit_transform(self._robust.fit_transform(\n",
    "        #         KNNImputer(weights='distance').fit_transform(X)))\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        with ThreadPoolExecutor(max_workers=len(self._models)) as executor:\n",
    "            res = executor.map(self._get_metrics, self._models)\n",
    "            results = [i for i in res if i]\n",
    "        return results\n",
    "\n",
    "    def fit(\n",
    "        self, X_train, y_train,\n",
    "        early_stopping: bool = False,\n",
    "        early_stopping_min_delta: float = 0.001,\n",
    "        early_stopping_patience: int = 10,\n",
    "        custom_models: list = None,\n",
    "        learning_rate: float = 0.05,\n",
    "        n_estimators: int = 100,\n",
    "        warm_start: bool = False,\n",
    "        complexity: bool = False,\n",
    "        light: bool = True,\n",
    "        custom_loss_metrics: object = False,\n",
    "    ):\n",
    "        \"\"\"_summary_\n",
    "        Args:\n",
    "            X_train (_type_): _description_\n",
    "            y_train (_type_): _description_\n",
    "            early_stopping (bool, optional): _description_. Defaults to False.\n",
    "            early_stopping_min_delta (float, optional): _description_. Defaults to 0.001.\n",
    "            early_stopping_patience (int, optional): _description_. Defaults to 10.\n",
    "            custom_models (list, optional): _description_. Defaults to None.\n",
    "            learning_rate (float, optional): _description_. Defaults to 0.05.\n",
    "            n_estimators (int, optional): _description_. Defaults to 100.\n",
    "            warm_start (bool, optional): _description_. Defaults to False.\n",
    "            complexity (bool, optional): _description_. Defaults to False.\n",
    "            custom_loss_metrics (object, optional): _description_. Defaults to False.\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        if custom_models:\n",
    "            self._models = custom_models\n",
    "        self.custom_loss_metrics = custom_loss_metrics\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.early_stopping = early_stopping\n",
    "        self.early_stopping_min_delta = early_stopping_min_delta\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        if custom_models:\n",
    "            self._models = custom_models\n",
    "        else:\n",
    "            if complexity:\n",
    "                self._models = (DecisionTreeRegressor, LinearRegression, BayesianRidge, KNeighborsRegressor, HistGradientBoostingRegressor,\n",
    "                                ElasticNet, LassoLars, Lasso, GradientBoostingRegressor, ExtraTreesRegressor,\n",
    "                                BaggingRegressor, NuSVR, XGBRegressor, SGDRegressor, KernelRidge, MLPRegressor, LGBMRegressor,\n",
    "                                Ridge, ARDRegression, RANSACRegressor, HuberRegressor, TheilSenRegressor, LassoLarsIC)\n",
    "            elif light:\n",
    "                self._models = (LGBMRegressor, ExtraTreesRegressor,\n",
    "                                BaggingRegressor, RANSACRegressor, LassoLarsIC, BayesianRidge)\n",
    "            else:\n",
    "                self._models = (DecisionTreeRegressor, LinearRegression, BayesianRidge, KNeighborsRegressor, LGBMRegressor,\n",
    "                                ElasticNet, LassoLars, Lasso, SGDRegressor, BaggingRegressor, ExtraTreesRegressor,\n",
    "                                Ridge, ARDRegression, RANSACRegressor, LassoLarsIC)\n",
    "        X_train = KNNImputer(weights='distance',\n",
    "                             n_neighbors=10).fit_transform(deepcopy(X_train))\n",
    "        self._y_mean = y_train.mean()\n",
    "        # base model: mean\n",
    "        # computer residuals: y - y hat\n",
    "        # for n_estimators: a) y = prev residuals && residuals * learning rate\n",
    "        # add early stopping\n",
    "        # restore best weights\n",
    "        # ada boost and adaptive scaling for learning rates\n",
    "\n",
    "        preds = DataFrame(\n",
    "            data={'yt': y_train, 'p0': np.full((len(y_train)), y_train - self._y_mean)})\n",
    "        residuals = DataFrame(\n",
    "            data={'r0': y_train - self._y_mean})\n",
    "        errors = []\n",
    "        if not early_stopping:\n",
    "            if warm_start:\n",
    "                for i in prange(1, self.n_estimators + 1):\n",
    "                    y = residuals[f'r{i - 1}']\n",
    "                    results = self._get_results(X_train, y)\n",
    "                    min_loss = min(results, key=lambda x: x.get(\n",
    "                        \"loss\", float('inf')))[\"loss\"]  # https://stackoverflow.com/a/19619294\n",
    "                    min_model = [i['model']\n",
    "                                 for i in results if min_loss >= i['loss']][0]\n",
    "                    preds[f'p{i}'] = residuals.sum(axis=1) + min_model.predict(\n",
    "                        X_train) * self.learning_rate\n",
    "                    residuals[f'r{i}'] = preds['yt'] - preds[f'p{i}']\n",
    "                    if i % 3 == 0:\n",
    "                        X_train[f\"r{i}\"] = residuals[f'r{i}'].copy()\n",
    "                    try:\n",
    "                        errors.append(mean_squared_error(\n",
    "                            preds['yt'], preds[f'p{i}']))\n",
    "                    except Exception:\n",
    "                        df = concat(\n",
    "                            [preds['yt'], preds[f'p{i - 1}']], axis=1).dropna()\n",
    "                        errors.append(mean_squared_error(\n",
    "                            df['yt'], df[f\"p{i - 1}\"]))\n",
    "                    self._ensemble.append(min_model)\n",
    "            else:\n",
    "                for i in prange(1, self.n_estimators + 1):\n",
    "                    y = residuals[f'r{i - 1}']\n",
    "                    results = self._get_results(X_train, y)\n",
    "                    min_loss = min(results, key=lambda x: x.get(\n",
    "                        \"loss\", float('inf')))[\"loss\"]  # https://stackoverflow.com/a/19619294\n",
    "                    min_model = [i['model']\n",
    "                                 for i in results if min_loss >= i['loss']][0]\n",
    "                    preds[f'p{i}'] = residuals.sum(axis=1) + min_model.predict(\n",
    "                        X_train) * self.learning_rate\n",
    "                    residuals[f'r{i}'] = preds['yt'] - preds[f'p{i}']\n",
    "                    errors.append(mean_squared_error(\n",
    "                        preds['yt'], preds[f'p{i}']))\n",
    "                    self._ensemble.append(min_model)\n",
    "                    if errors[i - 1] == 0:\n",
    "                        break\n",
    "        else:\n",
    "            return \"TODO\"\n",
    "        min_error = min(errors)\n",
    "        min_error_i = [i for i in prange(\n",
    "            len(errors)) if errors[i] == min_error][0]\n",
    "        self._ensemble, errors = self._ensemble[:\n",
    "                                                min_error_i], errors[:min_error_i]\n",
    "        residuals = residuals[:len(errors)]\n",
    "        return self._ensemble, (residuals, errors)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"_summary_\n",
    "        Args:\n",
    "            X_test (_type_): _description_\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        try:\n",
    "            val = self._ensemble[0]\n",
    "        except Exception:\n",
    "            return \"Please train the model first\"\n",
    "        # X_test = self._robust.transform(self._minimax.transform(deepcopy(X_test)))\n",
    "        preds = DataFrame(\n",
    "            data={'p0': np.full((len(X_test)), self._y_mean)})\n",
    "        for i in prange(len(self._ensemble)):\n",
    "            preds[f\"p{i}\"] = self._ensemble[i].predict(X_test)\n",
    "        preds_ = DataFrame(data={'preds': MinMaxScaler().fit_transform(preds.sum(axis=1).values.reshape(-1, 1)).reshape(1, -1)[0]}).values.reshape(1, -1)[0]\n",
    "        \n",
    "        @np.vectorize\n",
    "        def quantize(x):\n",
    "            if x > 0.5:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        return quantize(preds_)\n",
    "\n",
    "    def score(self, X_test, y_true):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_test (Iterable)\n",
    "            y_true (Iterable)\n",
    "        Returns:\n",
    "            float: R2 Score for y_true and y_predicted\n",
    "        \"\"\"\n",
    "        return r2_score(y_true, self.predict(X_test))\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 26 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X, y = make_classification(n_samples=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @np.vectorize\n",
    "# def quantize(x):\n",
    "#     if x > .5: return 0.8\n",
    "#     else: return .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = quantize(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(data={'y': (y)}).values.reshape(1, -1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# MinMaxScaler().fit_transform(y.reshape(-1, 1)).reshape(1, -1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 8.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, train_size=.7, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 9.61 s\n",
      "Wall time: 1.59 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8982115257231177, 0.9983832620066572)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from xgboost import XGBClassifier\n",
    "clf = XGBClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "(f1_score(y_val, clf.predict(X_val)), f1_score(y_train, clf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5min 38s\n",
      "Wall time: 1min 26s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9046261784696338, 0.9773223265964512)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf = VGBClassifier()\n",
    "_ = clf.fit(X_train, y_train,)\n",
    "(f1_score(y_val, clf.predict(X_val)), f1_score(y_train, clf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5min 34s\n",
      "Wall time: 1min 24s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9065897492788996, 0.9469890943575154)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import VGBoost\n",
    "clf = VGBoost.VGBClassifier()\n",
    "_ = clf.fit(X_train, y_train,)\n",
    "(f1_score(y_val, clf.predict(X_val)), f1_score(y_train, clf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.52 s\n",
      "Wall time: 276 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9043824701195219, 0.9653348446207615)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from lightgbm import LGBMClassifier\n",
    "clf = LGBMClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "(f1_score(y_val, clf.predict(X_val)), f1_score(y_train, clf.predict(X_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9539897755056679 > 0.9545251894783773"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "023b056bc26a3a4ec4ccb849012e9ec0a7e7339a1d248e7ddb8294afb0b6c3ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
