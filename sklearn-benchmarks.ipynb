{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from VGBoost import VGBRegressor, VGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.datasets import make_classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Make Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 328 ms\n",
      "Wall time: 360 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X, y = make_classification(n_samples=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 125 ms\n",
      "Wall time: 140 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, train_size=.7, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([49951, 50049], dtype=int64)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.bincount(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 9.95 s\n",
      "Wall time: 10.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8530022089831983, 1.0)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "(f1_score(y_val, clf.predict(X_val)), f1_score(y_train, clf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 59.3 s\n",
      "Wall time: 9.91 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9088675674778394, 0.9471622043667131)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from xgboost import XGBClassifier\n",
    "clf = XGBClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "(f1_score(y_val, clf.predict(X_val)), f1_score(y_train, clf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4min 4s\n",
      "Wall time: 45.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9110660775389522, 0.9245996757586962)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from lightgbm import LGBMRegressor\n",
    "clf = VGBClassifier()\n",
    "_ = clf.fit(X_train, y_train, custom_models=(LGBMRegressor,))\n",
    "(f1_score(y_val, clf.predict(X_val)), f1_score(y_train, clf.predict(X_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 19min 21s\n",
      "Wall time: 8min 41s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9107806691449815, 0.9246761372523926)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf = VGBClassifier()\n",
    "_ = clf.fit(X_train, y_train)\n",
    "(f1_score(y_val, clf.predict(X_val)), f1_score(y_train, clf.predict(X_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 156 ms\n",
      "Wall time: 153 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8766502200293373, 0.8761869457355499)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "(f1_score(y_val, clf.predict(X_val)), f1_score(y_train, clf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 16.7 s\n",
      "Wall time: 16.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9098683770962785, 1.0)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf = ExtraTreesClassifier()\n",
    "clf.fit(X_train, y_train, )\n",
    "(f1_score(y_val, clf.predict(X_val)), f1_score(y_train, clf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 8s\n",
      "Wall time: 1min 9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9121922081369881, 0.9175846014081308)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X_train, y_train, )\n",
    "(f1_score(y_val, clf.predict(X_val)), f1_score(y_train, clf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 52.2 s\n",
      "Wall time: 52.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9095872170439413, 0.9999571152883997)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train, )\n",
    "(f1_score(y_val, clf.predict(X_val)), f1_score(y_train, clf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.1 s\n",
      "Wall time: 15.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.910496365251143, 0.9150810628236885)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier()\n",
    "clf.fit(X_train, y_train, )\n",
    "(f1_score(y_val, clf.predict(X_val)), f1_score(y_train, clf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 25.3 s\n",
      "Wall time: 25.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9008275583664133, 0.993448686153361)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "clf = BaggingClassifier()\n",
    "clf.fit(X_train, y_train, )\n",
    "(f1_score(y_val, clf.predict(X_val)), f1_score(y_train, clf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Make Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=100000)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, train_size=.7,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4min 9s\n",
      "Wall time: 37.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(315.03532956342247, 192.05998759409934)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from xgboost import XGBRegressor\n",
    "clf = XGBRegressor()\n",
    "clf.fit(X_train, y_train)\n",
    "(mean_squared_error(y_val, clf.predict(X_val)), mean_squared_error(y_train, clf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7min 15s\n",
      "Wall time: 1min 24s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(187.21817424287443, 146.65307290915413)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "from VGBoost import  VGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "clf = VGBRegressor()\n",
    "_ = clf.fit(X_train, y_train, custom_models=(LGBMRegressor,))\n",
    "(mean_squared_error(y_val, clf.predict(X_val)), mean_squared_error(y_train, clf.predict(X_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4min 8s\n",
      "Wall time: 4min 10s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1097.5796499979313, 3.31799240136309e-26)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "clf = ExtraTreesRegressor()\n",
    "clf.fit(X_train, y_train, )\n",
    "(mean_squared_error(y_val, clf.predict(X_val)), mean_squared_error(y_train, clf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5min 5s\n",
      "Wall time: 5min 5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(573.9656580555924, 510.4399010938721)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "clf = GradientBoostingRegressor()\n",
    "clf.fit(X_train, y_train, )\n",
    "(mean_squared_error(y_val, clf.predict(X_val)), mean_squared_error(y_train, clf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10min 52s\n",
      "Wall time: 10min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1290.6340357359184, 181.40581486854506)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "clf = RandomForestRegressor()\n",
    "clf.fit(X_train, y_train, )\n",
    "(mean_squared_error(y_val, clf.predict(X_val)), mean_squared_error(y_train, clf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 39s\n",
      "Wall time: 1min 39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2666.6663827371067, 2591.1801157163245)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "clf = AdaBoostRegressor()\n",
    "clf.fit(X_train, y_train, )\n",
    "(mean_squared_error(y_val, clf.predict(X_val)), mean_squared_error(y_train, clf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 12s\n",
      "Wall time: 1min 12s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1577.9228713449559, 313.66391859064515)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "clf = BaggingRegressor()\n",
    "clf.fit(X_train, y_train, )\n",
    "(mean_squared_error(y_val, clf.predict(X_val)), mean_squared_error(y_train, clf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\aitfdl\\variational-gradient-boosting\\env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1h 28min 28s\n",
      "Wall time: 1h 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.2935759805544785e-25, 1.309912550141425e-25)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf = VGBRegressor()\n",
    "_ = clf.fit(X_train, y_train)\n",
    "(mean_squared_error(y_val, clf.predict(X_val)), mean_squared_error(y_train, clf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([RANSACRegressor(),\n",
       "  RANSACRegressor(),\n",
       "  RANSACRegressor(),\n",
       "  RANSACRegressor(),\n",
       "  LassoLarsIC(),\n",
       "  LassoLarsIC(),\n",
       "  RANSACRegressor(),\n",
       "  LassoLarsIC(),\n",
       "  LassoLarsIC(),\n",
       "  RANSACRegressor(),\n",
       "  RANSACRegressor(),\n",
       "  RANSACRegressor(),\n",
       "  RANSACRegressor(),\n",
       "  RANSACRegressor(),\n",
       "  RANSACRegressor(),\n",
       "  RANSACRegressor(),\n",
       "  RANSACRegressor(),\n",
       "  RANSACRegressor(),\n",
       "  BayesianRidge(),\n",
       "  RANSACRegressor(),\n",
       "  BayesianRidge(),\n",
       "  RANSACRegressor(),\n",
       "  BayesianRidge(),\n",
       "  BayesianRidge(),\n",
       "  BayesianRidge(),\n",
       "  RANSACRegressor(),\n",
       "  BayesianRidge(),\n",
       "  LGBMRegressor(),\n",
       "  BayesianRidge()],\n",
       " (            r0         r1         r2        r3        r4        r5        r6  \\\n",
       "  0   247.277988 -12.550822  12.991441 -1.277113  0.713428 -0.099527  0.040648   \n",
       "  1   -85.200758   4.073115  -4.463694  0.426840 -0.244527  0.033568 -0.013905   \n",
       "  2    67.454737  -3.559660   3.550720 -0.355519  0.195312 -0.027542  0.011143   \n",
       "  3  -140.305604   6.828357  -7.356698  0.709253 -0.403298  0.055628 -0.022946   \n",
       "  4    40.895426  -2.231694   2.156356 -0.219402  0.118788 -0.016910  0.006785   \n",
       "  5   165.549287  -8.464387   8.700684 -0.858254  0.477947 -0.066810  0.027238   \n",
       "  6   112.504584  -5.812152   5.915837 -0.586399  0.325112 -0.045576  0.018534   \n",
       "  7  -139.941417   6.810148  -7.337578  0.707386 -0.402248  0.055482 -0.022886   \n",
       "  8    80.567856  -4.215316   4.239159 -0.422724  0.233094 -0.032791  0.013294   \n",
       "  9    94.616659  -4.917756   4.976721 -0.494724  0.273572 -0.038415  0.015599   \n",
       "  10   49.270814  -2.650463   2.596064 -0.262326  0.142920 -0.020262  0.008159   \n",
       "  11 -132.729119   6.449533  -6.958933  0.670423 -0.381468  0.052595 -0.021703   \n",
       "  12  -65.336575   3.079906  -3.420824  0.325037 -0.187293  0.025616 -0.010645   \n",
       "  13 -111.292786   5.377717  -5.833525  0.560562 -0.319704  0.044013 -0.018186   \n",
       "  14 -119.880581   5.807106  -6.284384  0.604575 -0.344448  0.047451 -0.019595   \n",
       "  15  -28.055554   1.215855  -1.463570  0.133971 -0.079877  0.010692 -0.004528   \n",
       "  16 -104.780361   5.052095  -5.491623  0.527186 -0.300940  0.041406 -0.017117   \n",
       "  17  286.784448 -14.526145  15.065530 -1.479584  0.827256 -0.115342  0.047130   \n",
       "  18  -77.266422   3.676398  -4.047141  0.386177 -0.221666  0.030392 -0.012603   \n",
       "  19   24.350308  -1.404438   1.287737 -0.134609  0.071117 -0.010286  0.004070   \n",
       "  20  112.823539  -5.828100   5.932582 -0.588034  0.326031 -0.045703  0.018587   \n",
       "  21 -122.583363   5.942245  -6.426280  0.618426 -0.352235  0.048533 -0.020038   \n",
       "  22 -245.943352  12.110245 -12.902680  1.250646 -0.707666  0.097916 -0.040279   \n",
       "  23   27.269062  -1.550376   1.440972 -0.149567  0.079527 -0.011455  0.004549   \n",
       "  24   73.666656  -3.870256   3.876846 -0.387355  0.213210 -0.030028  0.012162   \n",
       "  25  -22.544941   0.940324  -1.174263  0.105729 -0.064000  0.008486 -0.003624   \n",
       "  26   13.019014  -0.837873   0.692844 -0.076536  0.038469 -0.005750  0.002211   \n",
       "  27 -106.104509   5.118303  -5.561141  0.533972 -0.304756  0.041936 -0.017335   \n",
       "  28  251.241117 -12.748979  13.199505 -1.297424  0.724846 -0.101114  0.041298   \n",
       "  \n",
       "            r7        r8        r9  ...           r21           r22  \\\n",
       "  0  -0.007009  0.002383 -0.000470  ... -3.109335e-11  7.872814e-12   \n",
       "  1   0.002374 -0.000814  0.000159  ...  1.058709e-11 -2.685852e-12   \n",
       "  2  -0.001934  0.000654 -0.000129  ... -8.554935e-12  2.160050e-12   \n",
       "  3   0.003929 -0.001344  0.000264  ...  1.747935e-11 -4.433787e-12   \n",
       "  4  -0.001185  0.000398 -0.000079  ... -5.222489e-12  1.321609e-12   \n",
       "  5  -0.004702  0.001597 -0.000315  ... -2.086153e-11  5.286438e-12   \n",
       "  6  -0.003205  0.001087 -0.000215  ... -1.419664e-11  3.595346e-12   \n",
       "  7   0.003918 -0.001340  0.000263  ...  1.745093e-11 -4.433787e-12   \n",
       "  8  -0.002304  0.000780 -0.000154  ... -1.020339e-11  2.586376e-12   \n",
       "  9  -0.002701  0.000915 -0.000181  ... -1.195133e-11  3.026912e-12   \n",
       "  10 -0.001421  0.000479 -0.000095  ... -6.266987e-12  1.584510e-12   \n",
       "  11  0.003715 -0.001271  0.000249  ...  1.654143e-11 -4.177991e-12   \n",
       "  12  0.001813 -0.000623  0.000122  ...  8.100187e-12 -2.046363e-12   \n",
       "  13  0.003110 -0.001065  0.000209  ...  1.385558e-11 -3.510081e-12   \n",
       "  14  0.003352 -0.001147  0.000225  ...  1.493561e-11 -3.794298e-12   \n",
       "  15  0.000761 -0.000264  0.000051  ...  3.421263e-12 -8.668621e-13   \n",
       "  16  0.002926 -0.001002  0.000196  ...  1.304556e-11 -3.311129e-12   \n",
       "  17 -0.008124  0.002763 -0.000544  ... -3.603873e-11  9.151790e-12   \n",
       "  18  0.002150 -0.000738  0.000144  ...  9.592327e-12 -2.430056e-12   \n",
       "  19 -0.000718  0.000239 -0.000048  ... -3.147704e-12  7.958079e-13   \n",
       "  20 -0.003214  0.001090 -0.000215  ... -1.423928e-11  3.609557e-12   \n",
       "  21  0.003429 -0.001173  0.000230  ...  1.527667e-11 -3.879563e-12   \n",
       "  22  0.006910 -0.002359  0.000463  ...  3.075229e-11 -7.787548e-12   \n",
       "  23 -0.000800  0.000267 -0.000053  ... -3.513634e-12  8.881784e-13   \n",
       "  24 -0.002110  0.000714 -0.000141  ... -9.322321e-12  2.359002e-12   \n",
       "  25  0.000606 -0.000211  0.000041  ...  2.732037e-12 -6.927792e-13   \n",
       "  26 -0.000398  0.000130 -0.000026  ... -1.726619e-12  4.369838e-13   \n",
       "  27  0.002964 -0.001015  0.000199  ...  1.320188e-11 -3.339551e-12   \n",
       "  28 -0.007121  0.002421 -0.000477  ... -3.160494e-11  8.014922e-12   \n",
       "  \n",
       "               r23           r24           r25           r26  r27  r28  r29  r30  \n",
       "  0  -1.961098e-12  4.831691e-13 -1.136868e-13  2.842171e-14  0.0  0.0  0.0  0.0  \n",
       "  1   6.536993e-13 -1.563194e-13  4.263256e-14 -1.421085e-14  0.0  0.0  0.0  0.0  \n",
       "  2  -5.400125e-13  1.421085e-13 -2.842171e-14  0.000000e+00  0.0  0.0  0.0  0.0  \n",
       "  3   1.108447e-12 -2.842171e-13  5.684342e-14  0.000000e+00  0.0  0.0  0.0  0.0  \n",
       "  4  -3.268497e-13  7.815970e-14 -2.131628e-14  7.105427e-15  0.0  0.0  0.0  0.0  \n",
       "  5  -1.307399e-12  3.126388e-13 -8.526513e-14  2.842171e-14  0.0  0.0  0.0  0.0  \n",
       "  6  -8.952838e-13  2.273737e-13 -5.684342e-14  1.421085e-14  0.0  0.0  0.0  0.0  \n",
       "  7   1.108447e-12 -2.842171e-13  5.684342e-14  0.000000e+00  0.0  0.0  0.0  0.0  \n",
       "  8  -6.394885e-13  1.563194e-13 -4.263256e-14  1.421085e-14  0.0  0.0  0.0  0.0  \n",
       "  9  -7.531753e-13  1.989520e-13 -5.684342e-14  1.421085e-14  0.0  0.0  0.0  0.0  \n",
       "  10 -3.907985e-13  9.947598e-14 -2.842171e-14  7.105427e-15  0.0  0.0  0.0  0.0  \n",
       "  11  1.023182e-12 -2.557954e-13  5.684342e-14  0.000000e+00  0.0  0.0  0.0  0.0  \n",
       "  12  4.973799e-13 -1.278977e-13  2.842171e-14  0.000000e+00  0.0  0.0  0.0  0.0  \n",
       "  13  8.668621e-13 -2.131628e-13  5.684342e-14 -1.421085e-14  0.0  0.0  0.0  0.0  \n",
       "  14  9.379164e-13 -2.273737e-13  5.684342e-14 -1.421085e-14  0.0  0.0  0.0  0.0  \n",
       "  15  2.131628e-13 -5.329071e-14  1.421085e-14 -3.552714e-15  0.0  0.0  0.0  0.0  \n",
       "  16  8.242296e-13 -2.131628e-13  5.684342e-14 -1.421085e-14  0.0  0.0  0.0  0.0  \n",
       "  17 -2.273737e-12  5.684342e-13 -1.136868e-13  0.000000e+00  0.0  0.0  0.0  0.0  \n",
       "  18  6.110668e-13 -1.563194e-13  4.263256e-14 -1.421085e-14  0.0  0.0  0.0  0.0  \n",
       "  19 -1.953993e-13  4.973799e-14 -1.421085e-14  3.552714e-15  0.0  0.0  0.0  0.0  \n",
       "  20 -8.952838e-13  2.273737e-13 -5.684342e-14  1.421085e-14  0.0  0.0  0.0  0.0  \n",
       "  21  9.663381e-13 -2.415845e-13  5.684342e-14 -1.421085e-14  0.0  0.0  0.0  0.0  \n",
       "  22  1.932676e-12 -4.831691e-13  1.136868e-13 -2.842171e-14  0.0  0.0  0.0  0.0  \n",
       "  23 -2.202682e-13  5.684342e-14 -1.421085e-14  3.552714e-15  0.0  0.0  0.0  0.0  \n",
       "  24 -5.826450e-13  1.421085e-13 -4.263256e-14  1.421085e-14  0.0  0.0  0.0  0.0  \n",
       "  25  1.705303e-13 -4.263256e-14  1.065814e-14 -3.552714e-15  0.0  0.0  0.0  0.0  \n",
       "  26 -1.083578e-13  2.664535e-14 -7.105427e-15  1.776357e-15  0.0  0.0  0.0  0.0  \n",
       "  27  8.242296e-13 -2.131628e-13  5.684342e-14 -1.421085e-14  0.0  0.0  0.0  0.0  \n",
       "  28 -1.989520e-12  4.831691e-13 -1.136868e-13  2.842171e-14  0.0  0.0  0.0  0.0  \n",
       "  \n",
       "  [29 rows x 31 columns],\n",
       "  [43.54753189512789,\n",
       "   47.97271979354862,\n",
       "   0.4572504711218865,\n",
       "   0.1444905977050735,\n",
       "   0.0027894488920177218,\n",
       "   0.00046857757691651106,\n",
       "   1.3861301416529045e-05,\n",
       "   1.6090533522714592e-06,\n",
       "   6.228902009804851e-08,\n",
       "   5.76127592779215e-09,\n",
       "   2.648439949327879e-10,\n",
       "   2.1241531531754786e-11,\n",
       "   1.0902364011972175e-12,\n",
       "   7.98909319601574e-14,\n",
       "   4.400953036569814e-15,\n",
       "   3.044841823132933e-16,\n",
       "   1.7551555459463154e-17,\n",
       "   1.1706083977565798e-18,\n",
       "   6.946918394060071e-20,\n",
       "   4.5260337856591594e-21,\n",
       "   2.7364825237461545e-22,\n",
       "   1.7563295542318683e-23,\n",
       "   1.0744041066386775e-24,\n",
       "   6.799902603892198e-26,\n",
       "   4.636796481958891e-27,\n",
       "   3.339786841962584e-28,\n",
       "   6.62098738036086e-35,\n",
       "   2.2214324314075186e-36,\n",
       "   4.836319614811697e-41]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 17.7 s\n",
      "Wall time: 18.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3877.385509782037, 0.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "clf = DecisionTreeRegressor()\n",
    "clf.fit(X_train, y_train, )\n",
    "(mean_squared_error(y_val, clf.predict(X_val)), mean_squared_error(y_train, clf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7 (tags/v3.8.7:6503f05, Dec 21 2020, 17:59:51) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "023b056bc26a3a4ec4ccb849012e9ec0a7e7339a1d248e7ddb8294afb0b6c3ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
